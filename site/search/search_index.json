{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"David Isaac Belais","text":"<p>505 SE 35th Ave, Portland OR 97214  |  503-267-0942  | david@belais.me | david.belais.me | github.com/davebelais</p>"},{"location":"#summary","title":"Summary","text":"<p>I am a highly productive data engineer and software engineer with 18 years of relevant experience.</p> <p>I pride myself in:</p> <ul> <li>Creating resilient, maintainable, integrous data pipelines and products     (ETL/ELT batch, micro-batch, and streaming for both OLAP and OLTP     databases/lake houses/data lakes/data warehouses) for \"big data\",     master data, fact data and dimensional data</li> <li>Authoring elegant, bulletproof, type-annotated, well-formed, thoroughly     tested, distributable Python libraries, CLIs, asynchronous micro-services     (web APIs), and SDKs</li> <li>Designing efficient, maintainable, testable, continuously integrated     and deployed, modern software systems (CI/CD, Test-driven development)</li> <li>Planning development work with clarity, flexibility, parallel execution,     and collaboration in mind (whether using Agile or waterfall)</li> <li>Leading engineering teams with complex and ambiguous directives towards     clear, executable road maps</li> <li>Condensing fact from the vapor of nuance while maintaining traceability and     continuity with data provenance, and facilitating data governance</li> <li>Utilizing \"AI\" and machine learning tools and frameworks selectively,     prudently, and with traceability and long-term cost efficacy in mind</li> <li>Infrastructure as Code: Terraform</li> </ul>"},{"location":"#skills","title":"Skills","text":"<p>I have professional experience with (not exhaustive):</p> <ul> <li>Platforms: Databricks, Snowflake, Amazon Web Services</li> <li>Languages: Python, SQL, C++, Javascript, HTML, XML, PHP, WSDL, Rust     (AWS - including Lambda, EMR, Aurora, IAM, Cloudformation, EC2, S3)</li> <li>OLTP Databases: PostgreSQL, MySQL, SQL Server, Oracle, IBM DB2, SQLite,     MariaDB</li> <li>OLAP \"Databases\" and query engines: Databricks Lakehouse, Deltalake,     Snowflake, Terradata, Netezza, Hive, Presto, DuckDB</li> <li>Applications, Services and Frameworks: Apache Spark, Apache Kafka,     SQLAlchemy, FastAPI, Flask, Docker, Terraform, Linux, Unix, Github Actions,     Jenkins, Kubernetes, Hadoop, Copilot</li> <li>Protocols and Specifications: Open API (Swagger), SOAP, MIME,     AS2 (for GDSN data pools), ASGI, WSGI</li> <li>Distributed File Systems: DBFS, S3, HDFS</li> </ul>"},{"location":"#experience","title":"Experience","text":""},{"location":"#nike-lead-data-engineer-sustainability-analytics-march-2021-june-2025","title":"Nike | Lead Data Engineer - Sustainability Analytics | March 2021 - June 2025","text":"**Platforms:** Databricks, Snowflake, Amazon Web Services     (AWS - including Lambda, EMR, Aurora, IAM, Cloudformation, EC2, S3) **Languages:** Python, SQL, Javascript **OLTP Databases:** PostgreSQL, Oracle, SQLite **OLAP \"Databases\" and query engines** Databricks Lakehouse, Deltalake,     Snowflake, Terradata, Hive, Presto **Applications, Services and Frameworks:** Apache Spark, Apache Kafka,     SQLAlchemy, FastAPI, Docker, Terraform, Linux, Github Actions,     Jenkins, Hadoop, Copilot **Protocols and Specifications:** Open API (Swagger), ASGI **Distributed File Systems:** DBFS, S3, HDFS   -   I lead engineering of the Nike Product &amp; Materials Sustainability Index     with data products published in Nike's \"Sole\" Databricks Platform     (Lakehouse/Deltalake/Unity Catalog),     Snowflake, and (prior to 2023) S3 + hive/presto + EMR. We utilized Spark,     Python (applications and libraries, distributed through JFrog Artifactory),     ASGI Microservices (Python + FastAPI + SQLAlchemy +     Alembic), Apache Kafka (as a publisher), and AWS Aurora PostgreSQL.     CI/CD using Jenkins and Github Actions, Terraform for     Infrastructure as Code.     This entailed design and implementation of systems for distilling     material manufacturing process lifecycle assessment data,     the expertise of the material scientists and sustainability professionals     with whom we collaborated, and materials data from our product creation     systems into data products attributing environmental impact     measures (greenhouse gas emissions and water quality     degradation/depletion) to Nike materials. We subsequently parsed     product bills of material line items in order to infer material gross usage     (way more involved than it sounds) in kilograms. Applying these measures     to purchase order and demand planning data we were able to measure and     track aggregate impacts for the enterprise. More importantly, we provided     tools for product developers to reduce environmental impacts through     better materials selection during the design process. To this end, we     stood up micro-services (Python + FastAPI on AWS lambda) for product and     material footprint scenario modeling.  -   I lead development of foundational data products exposing     Environmental Health &amp; Safety data from our 3rd-party EHS reporting system,     Enablon (via their \"Blink\" OData API), in Databricks Lakehouse (Unity     Catalog), Snowflake, and (prior to 2023) S3 + hive/presto + EMR. I used     Spark and Python.  -   I authored enterprise CLIs (command line interfaces) and frameworks (Python     libraries) for use in CI/CD and locally, numerous SDKs (software     development kits) for internal and third-party platforms, extended     SQLAlchemy and Alembic to facilitate use of ORMs     (object relational mappings) across multiple dialects simultaneously, and     to facilitate common and complex data frame operations in Spark, validate     data products based on ORM metadata, securely retrieve managed credentials,     and many other common development tasks."},{"location":"#bicp-nike-lead-data-engineer-sustainability-analytics-march-2020-march-2021","title":"BICP @ Nike | Lead Data Engineer - Sustainability Analytics | March 2020 - March 2021","text":"<p> Platforms: Snowflake, Amazon Web Services     (AWS - including Lambda, EMR, Aurora, IAM, Cloudformation, EC2, S3) Languages: Python, SQL, Rust OLTP Databases: PostgreSQL, Oracle, SQLite OLAP \"Databases\" and query engines: Snowflake, Terradata, Hive, Presto Applications, Services and Frameworks: Apache Spark,     SQLAlchemy, Docker, Terraform, Linux, Github Actions,     Jenkins, Hadoop Protocols and Specifications: Open API (Swagger) Distributed File Systems: S3, HDFS </p> <ul> <li>I developed a SQLAlchemy-ORM-based framework for automating deployment and     versioning (schema migration) supporting all database dialects leveraged     by the Nike Enterprise Data &amp; Analytics organization: Databricks,     Snowflake, Hive/Presto on S3, and PostgreSQL with full rollback     and versioning support.</li> <li>I authored a framework for Sustainability Analytics' ETL jobs incorporating     end-to-end schema-based data validations, local testing, and environment<ul> <li>file system abstraction.</li> </ul> </li> </ul>"},{"location":"#bicp-nike-senior-data-engineer-sustainability-analytics-january-2020-march-2020","title":"BICP @ Nike | Senior Data Engineer - Sustainability Analytics | January 2020 - March 2020","text":"<p> Platforms: Snowflake, Amazon Web Services     (AWS - including Lambda, EMR, Aurora, IAM, Cloudformation, EC2, S3) Languages: Python, SQL, Rust OLTP Databases: PostgreSQL, Oracle, SQLite OLAP \"Databases\" and query engines: Snowflake, Terradata, Hive, Presto Applications, Services and Frameworks: Apache Spark, SQLAlchemy, Docker,     Terraform, Linux, Jenkins, Hadoop Protocols and Specifications: Open API (Swagger), ASGI Distributed File Systems: S3, HDFS </p>"},{"location":"#the-kroger-co-lead-data-engineer-web-digital-analytics-may-2018-november-2019","title":"The Kroger Co. | Lead Data Engineer - Web &amp; Digital Analytics | May 2018 - November 2019","text":"<p> Languages: Python, SQL, Javascript, HTML, XML, WSDL OLTP Databases: SQL Server, IBM DB2, SQLite OLAP \"Databases\" and query engines: Netezza, Hive, Presto Applications, Services and Frameworks:  SQLAlchemy, Flask, Hadoop,     Magento Commerce, IBM Websphere Commerce Protocols and Specifications: Open API (Swagger), SOAP, MIME,     AS2 (for GDSN data pools), WSGI </p> <ul> <li>I lead development of data products distilling and exposing analytics to     buyers and planners correlating digital and store sales and EBITDA with     inventory,sell-through, prices, and promotional events\u2014contributing     to decisions resulting in a 56% increase in e-commerce sales in 2018 vs     2017, and a 67% increase in ecommerce sales in 2019 vs 2018.</li> <li>I lead development of pricing/promotions and product information     integration services for Magento Commerce.</li> </ul>"},{"location":"#the-kroger-co-lead-data-engineer-product-information-management-november-2013-may-2018","title":"The Kroger Co. | Lead Data Engineer - Product Information Management | November 2013 - May 2018","text":"<p> Languages: Python, SQL, Javascript, HTML, XML, WSDL OLTP Databases: SQL Server, IBM DB2, SQLite OLAP \"Databases\" and query engines: Netezza, Hive, Presto Applications, Services and Frameworks:  SQLAlchemy, Flask, Hadoop Protocols and Specifications: Open API (Swagger), SOAP, MIME,     AS2 (for GDSN data pools), WSGI Distributed File Systems: DBFS, S3, HDFS </p> <ul> <li>I lead development of multi-platform (Spark/Hive/Presto, Netezza, DB2,     Python, SQL Server, SQLAlchemy)     OLAP and OLTP data products to ingest, consolidate and normalize sales,     dimensional, and click-stream data from disparate subsidiary and partner     systems' transactional databases, streaming platforms, APIs, and     mainframes.</li> <li>I engineered algorithms for scoring semi-structured data and performing     human-in-the loop data validation and auditing for product descriptions,     specifications and photography acquired through trading partners     (Python, SQL).</li> <li>I established source-management capabilities for inbound data to handle     complex retailer/vendor/manufacturer relationships (Python, SQL).</li> <li>Collaborated with emerging digital initiatives to ensure the capture of all     metrics needed to facilitate accountability and continuous operational     improvement.</li> </ul>"},{"location":"#the-kroger-co-fred-meyer-stores-inc-business-systems-analyst-ecommerce-march-2011-november-2013","title":"The Kroger Co. (Fred Meyer Stores Inc.) | Business Systems Analyst - Ecommerce | March 2011 - November 2013","text":"<p> Languages: Python, SQL, Javascript, HTML, XML, WSDL OLTP Databases: SQL Server, IBM DB2, SQLite OLAP \"Databases\" and query engines: Netezza Applications, Services and Frameworks: SQLAlchemy, FastAPI, Flask Protocols and Specifications: Open API (Swagger), SOAP, MIME,     AS2 (for GDSN data pools), ASGI Distributed File Systems: DBFS, S3, HDFS </p> <ul> <li>I researched, designed, and prototyped Fred Meyer's (and later Kroger's)     product information management system for customer-facing digital     initiatives.</li> <li>I collaborated with Fred Meyer's technology partner, 1WorldSync, to     establish a roadmap, data model, and procedures for sourcing and validating     product data from GDSN data pools for     use in digital sales channels.</li> </ul>"},{"location":"#dissent-graphics-inc-full-stack-developer-january-2008-march-2011","title":"Dissent Graphics Inc. | Full-Stack Developer | January 2008 - March 2011","text":"<ul> <li>I designed and developed web applications for clients including:     The Garrigan Lyman Group, Microsoft, Best Buy, Avenue A Razorfish,     Nereus Communications, BlackEyedPeas.com, TeeFury.com, the Travel Channel\u2019s     Man v. Food, TheWho.com, Custom Rights, Hello Minor, ExoticTravelers.com,     and the ACLU of Oregon.</li> </ul>"},{"location":"#education","title":"Education","text":"<ul> <li>Portland State University | Computer Science (Postbaccalaureate) | 2018</li> <li>The Art Institute of Portland | Media Arts &amp; Animation | Bachelor of   Science | 2007</li> <li>Portland State University | Web Design | 2002 - 2004</li> <li>Loyola Marymount University | Fine Arts | 2000 - 2001</li> </ul>"},{"location":"#open-source-projects","title":"Open Source Projects","text":"<p>...because code examples are worth a thousand interview questions!</p> <ul> <li>git-author-stats:     A CLI and library for extracting periodic author \"stats\" (insertions and     deletions) for a Git repository or Github organization</li> <li>dependence:     A CLI and library for aligning a python projects' declared dependencies with the package versions installed in the environment in which dependence is executed, and for \"freezing\" recursively resolved package dependencies (like pip freeze, but for a package, instead of the entire environment).</li> <li>maya-zen-tools:     An Autodesk Maya extension providing modeling tools for     manipulating a polygon mesh using dynamically created NURBS curves and     surfaces to distribute vertices and/or UVs</li> <li>oapi: A python library for generating client     SDKs from Open API documents</li> <li>gittable: A CLI and library for     performing common, but complex, development and CI/CD tasks for a Git     repository, such as tagging a commit with your current project/package     version and downloading or accessing specific file(s) from a remote     repository (including non-public repos)</li> </ul> <p>Please see github.com/davebelais and github.com/enorganic for additional code examples.</p>"},{"location":"#certifications","title":"Certifications","text":"<ul> <li>JQL for Admins</li> <li>Jira Automation for Admins</li> <li>AWS Certified Big Data - Specialty</li> <li>AWS Certified Cloud Practitioner</li> </ul>"},{"location":"software_engineer_resume/","title":"David Isaac Belais","text":"<p>505 SE 35th Ave, Portland OR 97214  |  503-267-0942  | david@belais.me | david.belais.me | github.com/davebelais</p>"},{"location":"software_engineer_resume/#summary","title":"Summary","text":"<p>I am a highly productive data engineer and software engineer with 18 years of relevant experience.</p> <p>I pride myself in:</p> <ul> <li>Creating resilient, maintainable, integrous data pipelines and products     (ETL/ELT batch, micro-batch, and streaming for both OLAP and OLTP     databases/lake houses/data lakes/data warehouses) for \"big data\",     master data, fact data and dimensional data</li> <li>Authoring elegant, bulletproof, type-annotated, well-formed, thoroughly     tested, distributable Python libraries, CLIs, asynchronous micro-services     (web APIs), and SDKs</li> <li>Designing efficient, maintainable, testable, continuously integrated     and deployed, modern software systems (CI/CD, Test-driven development)</li> <li>Planning development work with clarity, flexibility, parallel execution,     and collaboration in mind (whether using Agile or waterfall)</li> <li>Leading engineering teams with complex and ambiguous directives towards     clear, executable road maps</li> <li>Condensing fact from the vapor of nuance while maintaining traceability and     continuity with data provenance, and facilitating data governance</li> <li>Utilizing \"AI\" and machine learning tools and frameworks selectively,     prudently, and with traceability and long-term cost efficacy in mind</li> <li>Infrastructure as Code: Terraform</li> </ul>"},{"location":"software_engineer_resume/#skills","title":"Skills","text":"<p>I have professional experience with (not exhaustive):</p> <ul> <li>Platforms: Databricks, Snowflake, Amazon Web Services</li> <li>Languages: Python, SQL, C++, Javascript, HTML, XML, PHP, WSDL, Rust     (AWS - including Lambda, EMR, Aurora, IAM, Cloudformation, EC2, S3)</li> <li>OLTP Databases: PostgreSQL, MySQL, SQL Server, Oracle, IBM DB2, SQLite,     MariaDB</li> <li>OLAP \"Databases\" and query engines: Databricks Lakehouse, Deltalake,     Snowflake, Terradata, Netezza, Hive, Presto, DuckDB</li> <li>Applications, Services and Frameworks: Apache Spark, Apache Kafka,     SQLAlchemy, FastAPI, Flask, Docker, Terraform, Linux, Unix, Github Actions,     Jenkins, Kubernetes, Hadoop, Copilot</li> <li>Protocols and Specifications: Open API (Swagger), SOAP, MIME,     AS2 (for GDSN data pools), ASGI, WSGI</li> <li>Distributed File Systems: DBFS, S3, HDFS</li> </ul>"},{"location":"software_engineer_resume/#experience","title":"Experience","text":""},{"location":"software_engineer_resume/#nike-lead-datasoftware-engineer-sustainability-analytics-march-2021-june-2025","title":"Nike | Lead Data/Software Engineer - Sustainability Analytics | March 2021 - June 2025","text":"**Platforms:** Databricks, Snowflake, Amazon Web Services     (AWS - including Lambda, EMR, Aurora, IAM, Cloudformation, EC2, S3) **Languages:** Python, SQL, Javascript **OLTP Databases:** PostgreSQL, Oracle, SQLite **OLAP \"Databases\" and query engines** Databricks Lakehouse, Deltalake,     Snowflake, Terradata, Hive, Presto **Applications, Services and Frameworks:** Apache Spark, Apache Kafka,     SQLAlchemy, FastAPI, Docker, Terraform, Linux, Github Actions,     Jenkins, Hadoop, Copilot **Protocols and Specifications:** Open API (Swagger), ASGI **Distributed File Systems:** DBFS, S3, HDFS   -   I lead engineering of the Nike Product &amp; Materials Sustainability Index     with data products published in Nike's \"Sole\" Databricks Platform     (Lakehouse/Deltalake/Unity Catalog),     Snowflake, and (prior to 2023) S3 + hive/presto + EMR. We utilized Spark,     Python (applications and libraries, distributed through JFrog Artifactory),     ASGI Microservices (Python + FastAPI + SQLAlchemy +     Alembic), Apache Kafka (as a publisher), and AWS Aurora PostgreSQL.     CI/CD using Jenkins and Github Actions, Terraform for     Infrastructure as Code.     This entailed design and implementation of systems for distilling     material manufacturing process lifecycle assessment data,     the expertise of the material scientists and sustainability professionals     with whom we collaborated, and materials data from our product creation     systems into data products attributing environmental impact     measures (greenhouse gas emissions and water quality     degradation/depletion) to Nike materials. We subsequently parsed     product bills of material line items in order to infer material gross usage     (way more involved than it sounds) in kilograms. Applying these measures     to purchase order and demand planning data we were able to measure and     track aggregate impacts for the enterprise. More importantly, we provided     tools for product developers to reduce environmental impacts through     better materials selection during the design process. To this end, we     stood up micro-services (Python + FastAPI on AWS lambda) for product and     material footprint scenario modeling.  -   I lead development of foundational data products exposing     Environmental Health &amp; Safety data from our 3rd-party EHS reporting system,     Enablon (via their \"Blink\" OData API), in Databricks Lakehouse (Unity     Catalog), Snowflake, and (prior to 2023) S3 + hive/presto + EMR. I used     Spark and Python.  -   I authored enterprise CLIs (command line interfaces) and frameworks (Python     libraries) for use in CI/CD and locally, numerous SDKs (software     development kits) for internal and third-party platforms, extended     SQLAlchemy and Alembic to facilitate use of ORMs     (object relational mappings) across multiple dialects simultaneously, and     to facilitate common and complex data frame operations in Spark, validate     data products based on ORM metadata, securely retrieve managed credentials,     and many other common development tasks."},{"location":"software_engineer_resume/#bicp-nike-lead-datasoftware-engineer-sustainability-analytics-march-2020-march-2021","title":"BICP @ Nike | Lead Data/Software Engineer - Sustainability Analytics | March 2020 - March 2021","text":"<p> Platforms: Snowflake, Amazon Web Services     (AWS - including Lambda, EMR, Aurora, IAM, Cloudformation, EC2, S3) Languages: Python, SQL, Rust OLTP Databases: PostgreSQL, Oracle, SQLite OLAP \"Databases\" and query engines: Snowflake, Terradata, Hive, Presto Applications, Services and Frameworks: Apache Spark,     SQLAlchemy, Docker, Terraform, Linux, Github Actions,     Jenkins, Hadoop Protocols and Specifications: Open API (Swagger) Distributed File Systems: S3, HDFS </p> <ul> <li>I developed a SQLAlchemy-ORM-based framework for automating deployment and     versioning (schema migration) supporting all database dialects leveraged     by the Nike Enterprise Data &amp; Analytics organization: Databricks,     Snowflake, Hive/Presto on S3, and PostgreSQL with full rollback     and versioning support.</li> <li>I authored a framework for Sustainability Analytics' ETL jobs incorporating     end-to-end schema-based data validations, local testing, and environment<ul> <li>file system abstraction.</li> </ul> </li> </ul>"},{"location":"software_engineer_resume/#bicp-nike-senior-datasoftware-engineer-sustainability-analytics-january-2020-march-2020","title":"BICP @ Nike | Senior Data/Software Engineer - Sustainability Analytics | January 2020 - March 2020","text":"<p> Platforms: Snowflake, Amazon Web Services     (AWS - including Lambda, EMR, Aurora, IAM, Cloudformation, EC2, S3) Languages: Python, SQL, Rust OLTP Databases: PostgreSQL, Oracle, SQLite OLAP \"Databases\" and query engines: Snowflake, Terradata, Hive, Presto Applications, Services and Frameworks: Apache Spark, SQLAlchemy, Docker,     Terraform, Linux, Jenkins, Hadoop Protocols and Specifications: Open API (Swagger), ASGI Distributed File Systems: S3, HDFS </p>"},{"location":"software_engineer_resume/#the-kroger-co-lead-datasoftware-engineer-web-digital-analytics-may-2018-november-2019","title":"The Kroger Co. | Lead Data/Software Engineer - Web &amp; Digital Analytics | May 2018 - November 2019","text":"<p> Languages: Python, SQL, Javascript, HTML, XML, WSDL OLTP Databases: SQL Server, IBM DB2, SQLite OLAP \"Databases\" and query engines: Netezza, Hive, Presto Applications, Services and Frameworks:  SQLAlchemy, Flask, Hadoop,     Magento Commerce, IBM Websphere Commerce Protocols and Specifications: Open API (Swagger), SOAP, MIME,     AS2 (for GDSN data pools), WSGI </p> <ul> <li>I lead development of data products distilling and exposing analytics to     buyers and planners correlating digital and store sales and EBITDA with     inventory,sell-through, prices, and promotional events\u2014contributing     to decisions resulting in a 56% increase in e-commerce sales in 2018 vs     2017, and a 67% increase in ecommerce sales in 2019 vs 2018.</li> <li>I lead development of pricing/promotions and product information     integration services for Magento Commerce.</li> </ul>"},{"location":"software_engineer_resume/#the-kroger-co-lead-datasoftware-engineer-product-information-management-november-2013-may-2018","title":"The Kroger Co. | Lead Data/Software Engineer - Product Information Management | November 2013 - May 2018","text":"<p> Languages: Python, SQL, Javascript, HTML, XML, WSDL OLTP Databases: SQL Server, IBM DB2, SQLite OLAP \"Databases\" and query engines: Netezza, Hive, Presto Applications, Services and Frameworks:  SQLAlchemy, Flask, Hadoop Protocols and Specifications: Open API (Swagger), SOAP, MIME,     AS2 (for GDSN data pools), WSGI Distributed File Systems: DBFS, S3, HDFS </p> <ul> <li>I lead development of multi-platform (Spark/Hive/Presto, Netezza, DB2,     Python, SQL Server, SQLAlchemy)     OLAP and OLTP data products to ingest, consolidate and normalize sales,     dimensional, and click-stream data from disparate subsidiary and partner     systems' transactional databases, streaming platforms, APIs, and     mainframes.</li> <li>I engineered algorithms for scoring semi-structured data and performing     human-in-the loop data validation and auditing for product descriptions,     specifications and photography acquired through trading partners     (Python, SQL).</li> <li>I established source-management capabilities for inbound data to handle     complex retailer/vendor/manufacturer relationships (Python, SQL).</li> <li>Collaborated with emerging digital initiatives to ensure the capture of all     metrics needed to facilitate accountability and continuous operational     improvement.</li> </ul>"},{"location":"software_engineer_resume/#the-kroger-co-fred-meyer-stores-inc-business-systems-analyst-ecommerce-march-2011-november-2013","title":"The Kroger Co. (Fred Meyer Stores Inc.) | Business Systems Analyst - Ecommerce | March 2011 - November 2013","text":"<p> Languages: Python, SQL, Javascript, HTML, XML, WSDL OLTP Databases: SQL Server, IBM DB2, SQLite OLAP \"Databases\" and query engines: Netezza Applications, Services and Frameworks: SQLAlchemy, FastAPI, Flask Protocols and Specifications: Open API (Swagger), SOAP, MIME,     AS2 (for GDSN data pools), ASGI Distributed File Systems: DBFS, S3, HDFS </p> <ul> <li>I researched, designed, and prototyped Fred Meyer's (and later Kroger's)     product information management system for customer-facing digital     initiatives.</li> <li>I collaborated with Fred Meyer's technology partner, 1WorldSync, to     establish a roadmap, data model, and procedures for sourcing and validating     product data from GDSN data pools for     use in digital sales channels.</li> </ul>"},{"location":"software_engineer_resume/#dissent-graphics-inc-full-stack-developer-january-2008-march-2011","title":"Dissent Graphics Inc. | Full-Stack Developer | January 2008 - March 2011","text":"<ul> <li>I designed and developed web applications for clients including:     The Garrigan Lyman Group, Microsoft, Best Buy, Avenue A Razorfish,     Nereus Communications, BlackEyedPeas.com, TeeFury.com, the Travel Channel\u2019s     Man v. Food, TheWho.com, Custom Rights, Hello Minor, ExoticTravelers.com,     and the ACLU of Oregon.</li> </ul>"},{"location":"software_engineer_resume/#education","title":"Education","text":"<ul> <li>Portland State University | Computer Science (Postbaccalaureate) | 2018</li> <li>The Art Institute of Portland | Media Arts &amp; Animation | Bachelor of   Science | 2007</li> <li>Portland State University | Web Design | 2002 - 2004</li> <li>Loyola Marymount University | Fine Arts | 2000 - 2001</li> </ul>"},{"location":"software_engineer_resume/#open-source-projects","title":"Open Source Projects","text":"<p>...because code examples are worth a thousand interview questions!</p> <ul> <li>git-author-stats:     A CLI and library for extracting periodic author \"stats\" (insertions and     deletions) for a Git repository or Github organization</li> <li>dependence:     A CLI and library for aligning a python projects' declared dependencies with the package versions installed in the environment in which dependence is executed, and for \"freezing\" recursively resolved package dependencies (like pip freeze, but for a package, instead of the entire environment).</li> <li>maya-zen-tools:     An Autodesk Maya extension providing modeling tools for     manipulating a polygon mesh using dynamically created NURBS curves and     surfaces to distribute vertices and/or UVs</li> <li>oapi: A python library for generating client     SDKs from Open API documents</li> <li>gittable: A CLI and library for     performing common, but complex, development and CI/CD tasks for a Git     repository, such as tagging a commit with your current project/package     version and downloading or accessing specific file(s) from a remote     repository (including non-public repos)</li> </ul> <p>Please see github.com/davebelais and github.com/enorganic for additional code examples.</p>"},{"location":"software_engineer_resume/#certifications","title":"Certifications","text":"<ul> <li>JQL for Admins</li> <li>Jira Automation for Admins</li> <li>AWS Certified Big Data - Specialty</li> <li>AWS Certified Cloud Practitioner</li> </ul>"}]}